From 20b2d6001bc48c332e9209b2f553f7b222091324 Mon Sep 17 00:00:00 2001
From: qianbi1999 <zhoujian157@huawei.com>
Date: Wed, 26 Nov 2025 09:55:04 +0800
Subject: [PATCH 1/3] Fix reward function not accessing num_turns

The reward function in `verl/experimental/reward/reward_loop/naive.py` was failing to retrieve the `num_turns` variable from the episode context.
This change ensures `num_turns` is properly passed to the reward calculation to maintain correct turn-based reward logic.
---
 verl/experimental/reward/reward_loop/naive.py | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/verl/experimental/reward/reward_loop/naive.py b/verl/experimental/reward/reward_loop/naive.py
index 7445cbdf..ba16d528 100644
--- a/verl/experimental/reward/reward_loop/naive.py
+++ b/verl/experimental/reward/reward_loop/naive.py
@@ -42,6 +42,8 @@ class NaiveRewardLoopManager(RewardLoopManagerBase):
         data_source = data_item.non_tensor_batch["data_source"]
         ground_truth = data_item.non_tensor_batch["reward_model"]["ground_truth"]
         extra_info = data_item.non_tensor_batch.get("extra_info", {})
+        num_turns = data_item.non_tensor_batch.get("__num_turns__", None)
+        extra_info["num_turns"] = num_turns
 
         response_str = await self.loop.run_in_executor(
             None, lambda: self.tokenizer.decode(valid_response_ids, skip_special_tokens=True)
-- 
2.50.1.windows.1

