From 676c01c0f53ab4d8a98268af978b0482ce35e808 Mon Sep 17 00:00:00 2001
From: caojingyi <caojingyi@noreply.gitcode.com>
Date: Wed, 12 Nov 2025 15:24:58 +0800
Subject: [PATCH 13/18] Update verl: megatron_utils.py
Enable EP to reshard parameters with AllToAllV (without communication redundancy) in `TP_extend_EP` training.
Optimize memory usage and communication performance via the expert parameter direcyed routing scheme.

---
 llm_rl/qwen3/verl/utils/megatron_utils.py | 195 +++++++++++++++++++-
 1 file changed, 194 insertions(+), 1 deletion(-)

diff --git a/llm_rl/qwen3/verl/utils/megatron_utils.py b/llm_rl/qwen3/verl/utils/megatron_utils.py
index 3eab811..b5d44ac 100644
--- a/llm_rl/qwen3/verl/utils/megatron_utils.py
+++ b/llm_rl/qwen3/verl/utils/megatron_utils.py
@@ -40,6 +40,7 @@ from verl.utils.device import get_device_id, get_device_name, get_torch_device
 from verl.utils.fs import local_mkdir_safe
 from verl.utils.model import normalize_model_name
 from verl.utils.torch_dtypes import PrecisionType
+from vllm.distributed.parallel_state import get_ep_group
 
 
 def get_model_config(model):
@@ -750,6 +751,12 @@ def default_tp_concat_fn(
     from megatron.core import mpu
 
     train_tp_size = mpu.get_tensor_model_parallel_world_size()
+
+    if hasattr(model_config, 'n_routed_experts'):
+        num_experts = model_config.n_routed_experts
+    elif hasattr(model_config, 'num_experts'):
+        num_experts = model_config.num_experts
+
     if layer_name_mapping.get("qkv_layer_name") in name and "layer_norm" not in name:
         # if the tensor is qkv, for each param on tp, split into q, k, v
         # concat q, k, v separately.
@@ -801,6 +808,36 @@ def default_tp_concat_fn(
         up = torch.cat(up_lst, dim=0)
         infer_params = torch.cat((gate, up), dim=0) if not convert_qkv_gate_up_by_simple_split else [gate, up]
 
+    elif "mlp.experts.weight1" in name:     # for moe group matmul
+        gate_pp_lst = []
+        up_pp_lst = []
+        if os.getenv('ALL_TO_ALL_RESHARD', '0') == '0':
+            for infer_param in infer_params:
+                split_size = [
+                    model_config.moe_intermediate_size,
+                    model_config.moe_intermediate_size,
+                ] * (num_experts // mpu.get_expert_tensor_and_model_parallel_world_size())
+                experts_weight = infer_param.split(split_size, dim=1)
+                gate_pp_lst.extend(experts_weight[::2])
+                up_pp_lst.extend(experts_weight[1::2])
+            infer_params = [tensor.transpose(0, 1) for pair in zip(gate_pp_lst, up_pp_lst) for tensor in pair]
+        else:
+            # To optimize memory, only the params for the current rank are non-empty; others are empty tensors
+            infer_params = get_rollout_expert_after_resharding(infer_params, model_config, is_weight1=True)
+    elif "mlp.experts.weight2" in name:     # for moe group matmul
+        down_pp_lst = []
+        if os.getenv('ALL_TO_ALL_RESHARD', '0') == '0':
+            for infer_param in infer_params:
+                split_size = [
+                    model_config.moe_intermediate_size
+                ] * (num_experts // mpu.get_expert_tensor_and_model_parallel_world_size())
+                experts_weight = infer_param.split(split_size, dim=0)
+                down_pp_lst.extend(experts_weight)
+            experts_down_pp = [downs.transpose(0, 1) for downs in down_pp_lst]
+            infer_params = experts_down_pp
+        else:
+            # To optimize memory, only the params for the current rank are non-empty; others are empty tensors
+            infer_params = get_rollout_expert_after_resharding(infer_params, model_config, is_weight1=False)
     elif "mlp.experts.linear_fc2.weight" in name:  # moe
         infer_params = torch.cat(infer_params, dim=1)
 
@@ -829,6 +866,7 @@ def per_tensor_generator(
     vpp_size = len(actor_module)
     all_gather_group = mpu.get_tensor_model_parallel_group()
     all_gather_group_size = torch.distributed.get_world_size(group=all_gather_group)
+    etmp_group = mpu.get_expert_tensor_and_model_parallel_group()
 
     def tensor_generator():
         for scan_vpp_idx in range(vpp_size):
@@ -928,6 +966,34 @@ def per_tensor_generator(
 
                 yield from zip(converted_names, [param.detach() for param in converted_params], strict=True)
             continue
+        
+        elif ".mlp.experts.weight" in cur_name and ep_size > 1:
+            if etp_size > 1:
+                raise NotImplementedError("Reshard for ETP params when using MoE Group Matmul not supported for now.")
+            if os.getenv('ALL_TO_ALL_RESHARD', '0') == '0':
+                ep_params = [torch.empty_like(broad_pp_tensor) for _ in range(ep_size)]
+                torch.distributed.all_gather(ep_params, broad_pp_tensor, group=etmp_group)
+            else:
+                # EP param reshard method based on AllToAllV, efficient in both memory usage and communication performance
+                ep_params = ep_param_reshard_by_alltoallv(
+                    param_name=cur_name,
+                    ep_param_train=broad_pp_tensor,
+                    num_experts=weight_converter.mcore_config.num_moe_experts,
+                    weight1_key_name="mlp.experts.weight1",
+                    weight2_key_name="mlp.experts.weight2"
+                )
+            merge_params = default_tp_concat_fn(
+                layer_name_mapping,
+                cur_name,
+                broad_pp_tensor,
+                ep_params,
+                model_config,
+                convert_qkv_gate_up_by_simple_split
+            )
+            converted_names, converted_params = weight_converter.convert_param(cur_name, merge_params)
+
+            yield from zip(converted_names, converted_params)
+            continue
 
         # tp all gather
         if tp_utils.is_tensor_parallel_param(broad_pp_tensor):
@@ -955,7 +1021,6 @@ def per_tensor_generator(
 
         yield from zip(converted_names, [param.detach() for param in converted_params], strict=True)
 
-
 def get_transformer_layer_offset(pipeline_rank, vp_stage, config: TransformerConfig):
     """
     Get the index offset of any pipeline stage, given the level of pipelining.
@@ -1105,3 +1170,131 @@ def get_transformer_layer_offset(pipeline_rank, vp_stage, config: TransformerCon
     else:
         offset = 0
     return offset
+
+def ep_param_reshard_by_alltoallv(
+    param_name,
+    ep_param_train,
+    num_experts,
+    weight1_key_name="mlp.experts.weight1",
+    weight2_key_name="mlp.experts.weight2"
+):
+    """Reshard EP params by AllToAllV for better memory usage and communication performance in TP_extend_EP training
+
+    Args:
+        param_name: EP param name in the training engine
+        ep_param_train: EP param shard held by this rank in the training engine
+        num_experts: total number of routing experts in the complete model
+        weight1_key_name: key word for the expert weight1 name in the training engine
+        weight2_key_name: key word for the expert weight2 name in the training engine
+
+    For example, Train EP4PP2 and Rollout EP8PP1, after PP allgather in veRL, the communication is like below:
+    train ep ranks:     0    1    2    3  |  0    1    2    3
+                        | \    \                      /    /|
+                        |   \    \----\       /-----/    /  |
+    rollout ep ranks:   0    1    2    3     4    5    6    7
+
+    the send tensors for global rank 0 is: [shard_to_rank0, shard_to_rank1, empty, empty]
+    the recv tensors for global rank 0 is: [shard_from_rank0, empty, empty, empty]
+    the send tensors for global rank 4 is: [empty, empty, empty, empty]
+    the recv tensors for global rank 4 is: [empty, empty, shard_from_rank6, empty]
+    """
+    ep_size_train = mpu.get_expert_tensor_and_model_parallel_world_size()
+    ep_rank_train = mpu.get_expert_tensor_and_model_parallel_rank()
+    ep_group_rollout = get_ep_group().device_group
+    ep_size_rollout = torch.distributed.get_world_size(ep_group_rollout)
+    ep_rank_rollout = torch.distributed.get_rank(group=ep_group_rollout)
+    assert ep_size_rollout % ep_size_train == 0, f"EP size of rollout {ep_size_rollout} must be divisible by EP size of training{ep_size_train}"
+    micro_ep_size = ep_size_rollout // ep_size_train
+
+    assert num_experts % ep_size_train == 0 and num_experts % ep_size_rollout == 0
+    num_experts_train = num_experts // ep_size_train
+    num_experts_rollout = num_experts // ep_size_rollout
+
+    if weight1_key_name in param_name:
+        hidden_size = ep_param_train.shape[0]
+        # The actual memory layout of weight `w13` is [num_experts_train, hidden_size, moe_intermediate_size],
+        # view the tensor to a correct shape before using it.
+        # Also, training phase and rollout phase expect different layouts for `w13`, with inversed dimension
+        # order of `hidden_size` and `moe_intermediate_size`, necessiting the `transpose` and `contiguous` here.
+        ep_param_train = ep_param_train.view(num_experts_train, hidden_size, -1).transpose(1, 2).contiguous()
+
+        split_size = num_experts_train // micro_ep_size
+        rollout_weight_shape = [split_size, ep_param_train.shape[1], hidden_size]
+
+    elif weight2_key_name in param_name:
+        hidden_size = ep_param_train.shape[1]
+        # Similar to the handling of `w13`.
+        ep_param_train = ep_param_train.view(num_experts_train, -1, hidden_size).transpose(1, 2).contiguous()
+
+        split_size = num_experts_train // micro_ep_size
+        rollout_weight_shape = [split_size, hidden_size, ep_param_train.shape[2]]
+    else:
+        raise NotImplementedError(f"Weight {param_name} not supported in EP param resharding yet!")
+
+    # for send: get the corresponding rollout ep ranks of this training ep group
+    ep_train_group_idx = ep_rank_rollout // ep_size_train    # train ep group idx within the larger rollout ep group of this rank
+    ep_rank_range_rollout = list(range(ep_train_group_idx * ep_size_train, ep_train_group_idx * ep_size_train + ep_size_train, 1))
+    # for recv: get the src rollout ep rank of this rank
+    recv_src_rank = ep_rank_rollout // micro_ep_size
+    send_tensors = []   # sharded ep params to send to each rank in this training ep group by this rank
+    recv_tensors = []   # recv buffers for this rank to recv sharded ep params from each rank in this training ep group
+    split_start_idx = 0
+    for rank_ep_train in range(ep_size_train):
+        # update send_tensors
+        rank_ep_rollout = ep_rank_range_rollout[rank_ep_train]
+        if rank_ep_rollout // micro_ep_size == ep_rank_train:
+            tensor_to_send = ep_param_train[split_start_idx:split_start_idx + split_size, ...]
+            send_tensors.append(tensor_to_send)
+            split_start_idx += split_size
+        else:
+            send_tensors.append(torch.zeros(0, dtype=ep_param_train.dtype, device=ep_param_train.device))   # placeholder
+
+        # update recv_tensors
+        if recv_src_rank == rank_ep_train:
+            recv_tensors.append(torch.empty(rollout_weight_shape, dtype=ep_param_train.dtype, device=ep_param_train.device))
+        else:
+            recv_tensors.append(torch.empty(0, dtype=ep_param_train.dtype, device=ep_param_train.device))   # placeholder
+
+    torch.distributed.all_to_all(recv_tensors, send_tensors, group=mpu.get_expert_tensor_and_model_parallel_group())
+    # filter out empty tensors and retain only the ep params required by this rank in rollout
+    ep_params = [param for param in recv_tensors if param.numel() > 0]
+    return ep_params
+
+
+def get_rollout_expert_after_resharding(infer_params, model_config, is_weight1):
+    """Postprocess the resharded EP parameter to return EP parameters for all ranks.
+    To optimize memory, only the params for the current rank are non-empty; others are empty tensors.
+
+    Args:
+        infer_params: a tensor list but only contains one ep param of weight1 for this rank in rollout
+        model_config: hugging face model config
+    """
+    assert len(infer_params) == 1
+    rollout_ep_group = get_ep_group().device_group
+    rollout_ep_size = torch.distributed.get_world_size(rollout_ep_group)
+    ep_rank_rollout = torch.distributed.get_rank(group=rollout_ep_group)
+
+    if hasattr(model_config, 'n_routed_experts'):
+        num_experts = model_config.n_routed_experts
+    elif hasattr(model_config, 'num_experts'):
+        num_experts = model_config.num_experts
+    else:
+        raise NotImplementedError("The current method of obtaining num_experts is not supported yet.")
+    num_experts_rollout = num_experts // rollout_ep_size
+
+    # expert ids held by current rank in rollout
+    local_expert_ids = list(range(ep_rank_rollout * num_experts_rollout, ep_rank_rollout * num_experts_rollout + num_experts_rollout))
+
+    local_expert_params = infer_params[0]
+    if is_weight1:
+        experts_gate_pp = [torch.empty(0, dtype=local_expert_params[0].dtype, device=local_expert_params[0].device) for idx in range(num_experts)]
+        experts_up_pp = [torch.empty(0, dtype=local_expert_params[0].dtype, device=local_expert_params[0].device) for idx in range(num_experts)]
+        for local_idx, expert_id in enumerate(local_expert_ids):
+            experts_gate_pp[expert_id], experts_up_pp[expert_id] = torch.chunk(local_expert_params[local_idx], chunks=2, dim=0)
+        infer_params = [tensor for pair in zip(experts_gate_pp, experts_up_pp) for tensor in pair]
+        return infer_params
+    else:
+        experts_down_pp = [torch.empty(0, dtype=local_expert_params[0].dtype, device=local_expert_params[0].device) for idx in range(num_experts)]
+        for local_idx, expert_id in enumerate(local_expert_ids):
+            experts_down_pp[expert_id] = local_expert_params[local_idx]
+        return experts_down_pp
-- 
2.50.1.windows.1

