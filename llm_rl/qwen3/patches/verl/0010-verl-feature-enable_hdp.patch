From 478f2dab30e96b26707bb57b3607f49791e39293 Mon Sep 17 00:00:00 2001
From: qushiyi <qushiyi@huawei.com>
Date: Wed, 26 Nov 2025 15:16:55 +0800
Subject: [PATCH] verl-feature-enable_hdp

---
 llm_rl/qwen3/verl/models/mcore/model_forward.py | 12 ++++++++++++
 llm_rl/qwen3/verl/utils/seqlen_balancing.py     |  9 ++++++++-
 2 files changed, 20 insertions(+), 1 deletion(-)

diff --git a/llm_rl/qwen3/verl/models/mcore/model_forward.py b/llm_rl/qwen3/verl/models/mcore/model_forward.py
index a6e5489..76330b2 100644
--- a/llm_rl/qwen3/verl/models/mcore/model_forward.py
+++ b/llm_rl/qwen3/verl/models/mcore/model_forward.py
@@ -14,6 +14,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import os
 from verl.utils.megatron_utils import unwrap_model
 
 from .util import (
@@ -41,6 +42,17 @@ def gptmodel_forward(
     """Default forward pass for GPT models with optional sequence packing."""
     pre_process = unwrap_model(model).pre_process
     post_process = unwrap_model(model).post_process
+
+    global preprocess_packed_seqs
+    global postprocess_packed_seqs
+
+    if os.environ.get("USE_HDP") == "1":
+        from patches.verl.utils.hybrid_data_parallel.hdp import (preprocess_packed_seqs_hdp, postprocess_packed_seqs_hdp,
+                                                        generate_hdp_group_from_batch)
+        generate_hdp_group_from_batch(attention_mask)
+        preprocess_packed_seqs = preprocess_packed_seqs_hdp
+        postprocess_packed_seqs = postprocess_packed_seqs_hdp
+
     if pack_seqs:
         batch_size, seq_len = attention_mask.shape[:2]
         input_ids_rmpad, packed_seq_params = preprocess_packed_seqs(input_ids, attention_mask, pre_process=pre_process)
diff --git a/llm_rl/qwen3/verl/utils/seqlen_balancing.py b/llm_rl/qwen3/verl/utils/seqlen_balancing.py
index 5354d51..1f5d834 100644
--- a/llm_rl/qwen3/verl/utils/seqlen_balancing.py
+++ b/llm_rl/qwen3/verl/utils/seqlen_balancing.py
@@ -12,6 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import os
 import copy
 import heapq
 from itertools import chain
@@ -23,6 +24,8 @@ from verl.protocol import DataProto
 from verl.utils import tensordict_utils as tu
 from verl.utils.device import get_device_name
 
+from patches.verl.utils.hybrid_data_parallel.hdp import pack_sequences_into_buckets
+
 
 def karmarkar_karp(seqlen_list: list[int], k_partitions: int, equal_size: bool):
     # see: https://en.wikipedia.org/wiki/Largest_differencing_method
@@ -301,7 +304,11 @@ def rearrange_micro_batches(
     seq_len_effective = seq_len_effective.tolist()
     assert num_micro_batches <= len(seq_len_effective)
 
-    micro_bsz_idx = get_seqlen_balanced_partitions(seq_len_effective, num_micro_batches, equal_size=False)
+    if os.environ.get("USE_HDP") == "1":
+        max_token_len = ceildiv(total_seqlen, num_micro_batches)
+        micro_bsz_idx = pack_sequences_into_buckets(seq_len_effective, max_token_len, num_micro_batches)
+    else:
+        micro_bsz_idx = get_seqlen_balanced_partitions(seq_len_effective, num_micro_batches, equal_size=False)
 
     if use_dynamic_bsz_balance:
         # Use the sum of squared sequence lengths to approximate attention computation workload
-- 
2.50.1.windows.1

