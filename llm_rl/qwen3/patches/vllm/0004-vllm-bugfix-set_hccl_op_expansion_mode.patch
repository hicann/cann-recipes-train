From a935ee32623bb84d7a131808a824787720612f32 Mon Sep 17 00:00:00 2001
From: mystri <hanboyou@huawei.com>
Date: Mon, 15 Dec 2025 21:46:07 +0800
Subject: [PATCH] Avoided hccl timeout of all_gather-ing from multiple
 ProcessGroups by setting hccl_op_expansion_mode

---
 llm_rl/qwen3/vllm/distributed/parallel_state.py | 14 ++++++++++++--
 1 file changed, 12 insertions(+), 2 deletions(-)

diff --git a/llm_rl/qwen3/vllm/distributed/parallel_state.py b/llm_rl/qwen3/vllm/distributed/parallel_state.py
index 6381709..3d8625d 100644
--- a/llm_rl/qwen3/vllm/distributed/parallel_state.py
+++ b/llm_rl/qwen3/vllm/distributed/parallel_state.py
@@ -205,6 +205,7 @@ class GroupCoordinator:
         use_device_communicator: bool,  # whether to use device communicator
         use_message_queue_broadcaster: bool = False,
         group_name: Optional[str] = None,
+        pg_options: Optional[dict] = None,
     ):
         group_name = group_name or "anonymous"
         self.unique_name = _get_unique_name(group_name)
@@ -218,7 +219,7 @@ class GroupCoordinator:
 
         for ranks in group_ranks:
             device_group = torch.distributed.new_group(
-                ranks, backend=torch_distributed_backend)
+                ranks, backend=torch_distributed_backend, pg_options=pg_options)
             # a group with `gloo` backend, to allow direct coordination between
             # processes through the CPU.
             cpu_group = torch.distributed.new_group(ranks, backend="gloo")
@@ -919,6 +920,7 @@ def init_model_parallel_group(
     backend: str,
     use_message_queue_broadcaster: bool = False,
     group_name: Optional[str] = None,
+    pg_options: Optional[dict] = None,
 ) -> GroupCoordinator:
 
     return GroupCoordinator(
@@ -928,6 +930,7 @@ def init_model_parallel_group(
         use_device_communicator=True,
         use_message_queue_broadcaster=use_message_queue_broadcaster,
         group_name=group_name,
+        pg_options=pg_options,
     )
 
 
@@ -1148,12 +1151,19 @@ def initialize_model_parallel(
     group_ranks = all_ranks.view(-1, tensor_model_parallel_size).unbind(0)
     group_ranks = [x.tolist() for x in group_ranks]
 
+    from torch.distributed import is_hccl_available
+    from torch_npu._C._distributed_c10d import ProcessGroupHCCL
+    assert is_hccl_available()
+    options = ProcessGroupHCCL.Options()
+    options.hccl_config = {"hccl_op_expansion_mode": 3}
+
     # message queue broadcaster is only used in tensor model parallel group
     _TP = init_model_parallel_group(group_ranks,
                                     get_world_group().local_rank,
                                     backend,
                                     use_message_queue_broadcaster=True,
-                                    group_name="tp")
+                                    group_name="tp",
+                                    pg_options=options)
 
     # Build the DCP model-parallel groups.
     global _DCP
-- 
2.28.0.windows.1

