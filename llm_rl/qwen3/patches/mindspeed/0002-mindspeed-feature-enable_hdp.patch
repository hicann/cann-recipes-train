From f7b0263435ad12c158811c9248d197db427fa8c9 Mon Sep 17 00:00:00 2001
From: qushiyi <qushiyi@huawei.com>
Date: Wed, 26 Nov 2025 15:52:56 +0800
Subject: [PATCH] mindspeed-feature-enable_hdp

---
 .../ring_context_parallel/ring_context_parallel.py   | 12 ++++++++++++
 .../qwen3/mindspeed/core/context_parallel/utils.py   |  4 ++++
 2 files changed, 16 insertions(+)

diff --git a/llm_rl/qwen3/mindspeed/core/context_parallel/ring_context_parallel/ring_context_parallel.py b/llm_rl/qwen3/mindspeed/core/context_parallel/ring_context_parallel/ring_context_parallel.py
index c545e00..cc4095c 100644
--- a/llm_rl/qwen3/mindspeed/core/context_parallel/ring_context_parallel/ring_context_parallel.py
+++ b/llm_rl/qwen3/mindspeed/core/context_parallel/ring_context_parallel/ring_context_parallel.py
@@ -11,6 +11,7 @@ from mindspeed.ops.fusion_attention_v2 import npu_fusion_attention, npu_fusion_a
 from mindspeed.core.context_parallel import get_args
 from mindspeed.core.context_parallel.ring_context_parallel.context_parallel_kv_cache import ContextParallelKVCache
 from mindspeed.core.context_parallel.utils import RingP2P, tnd_out_update, causal_out_update, general_out_update, forward_update, unflatten_softmax, flatten_softmax, get_selection_indices_for_tnd_softmax_update
+from patches.verl.utils.hybrid_data_parallel.utils import get_batch_hdp_group
 
 
 def causal_forward_fetch(q_block_id, kv_block_id, q, cur_k, cur_v, attn_mask=None):
@@ -1213,6 +1214,17 @@ class AttentionWithCp(torch.autograd.Function):
 
 def ringattn_context_parallel(q, k, v, n, cp_para, softmax_scale=None, attn_mask=None, dropout_p=0.,
                               packed_seq_params=None, shapes=None):
+    batch_hdp_group = get_batch_hdp_group()
+    hdp_group = next(group for group in batch_hdp_group if cp_para['rank'] in group) if batch_hdp_group else None
+    if hdp_group is not None:
+        rank = cp_para['rank']
+        cp_global_ranks = cp_para['cp_global_ranks']
+        cp_para['rank'] = hdp_group.index(rank)
+        cp_para['cp_size'] = len(hdp_group)
+        cp_para['cp_global_ranks'] = [cp_global_ranks[i] for i in hdp_group]
+        cp_para['cp_outer_ranks'] = cp_para['cp_global_ranks']
+        cp_para['cp_dkv_outer_ranks'] = cp_para['cp_global_ranks']
+
     AttentionWithCp.block_size = q.shape[0]
     AttentionWithCp.batch_size = q.shape[1]
     out = AttentionWithCp.apply(
diff --git a/llm_rl/qwen3/mindspeed/core/context_parallel/utils.py b/llm_rl/qwen3/mindspeed/core/context_parallel/utils.py
index 4d96365..0046388 100644
--- a/llm_rl/qwen3/mindspeed/core/context_parallel/utils.py
+++ b/llm_rl/qwen3/mindspeed/core/context_parallel/utils.py
@@ -136,6 +136,10 @@ class RingP2P:
             self.next, self.prev = self.prev, self.next
 
         self.send_recv_ops = []
+        
+        ring_rank_prev = ring_global_ranks.index(self.prev)
+        if self.ring_rank % 2 == 0 and ring_rank_prev % 2 == 0:
+            self.group_for_send_recv_overlap = group
 
     def async_send_recv(self, send_tensor, recv_tensor, shapes=None):
         if self.ring_rank % 2 == 0:
-- 
2.50.1.windows.1

