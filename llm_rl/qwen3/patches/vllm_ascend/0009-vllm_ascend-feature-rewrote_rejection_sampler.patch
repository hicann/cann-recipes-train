From 8beb68e7bbbd043f0a1d3f7b07c255e00556db39 Mon Sep 17 00:00:00 2001
From: mystri <mystri@noreply.gitcode.com>
Date: Tue, 2 Dec 2025 00:36:11 +0800
Subject: [PATCH] Rewrote rejection sampler with efficient tensor operations

---
 qwen3/vllm_ascend/sample/rejection_sampler.py | 231 ++++++++++++------
 1 file changed, 162 insertions(+), 69 deletions(-)

diff --git a/llm_rl/qwen3/vllm_ascend/sample/rejection_sampler.py b/llm_rl/qwen3/vllm_ascend/sample/rejection_sampler.py
index e0d770d..7611183 100644
--- a/llm_rl/qwen3/vllm_ascend/sample/rejection_sampler.py
+++ b/llm_rl/qwen3/vllm_ascend/sample/rejection_sampler.py
@@ -237,16 +237,18 @@ def expand_batch_to_tokens(
         expanded_x: [num_tokens] tensor.
     """
     batch_size = x.shape[0]
-    assert cu_num_tokens.shape[0] == batch_size
-    expanded_x = x.new_empty(num_tokens)
-    expand_pytorch(
-        expanded_x,
-        x,
-        cu_num_tokens,
-        replace_from,
-        replace_to,
-        MAX_NUM_TOKENS=MAX_SPEC_LEN,  # To avoid recompilation.
-    )
+    if batch_size == 0:
+        return x.new_empty(num_tokens)
+    
+    zero = torch.tensor([0], device=cu_num_tokens.device, dtype=cu_num_tokens.dtype)
+    cu_num_tokens_with_zero = torch.cat((zero, cu_num_tokens))
+    lengths = torch.diff((cu_num_tokens_with_zero))
+
+    src_vals = x
+    if replace_from != replace_to:
+        src_vals = torch.where(x == replace_from, replace_to, x)
+    
+    expanded_x = torch.repeat_interleave(src_vals, repeats=lengths, dim=0)
     return expanded_x
 
 
@@ -392,46 +394,141 @@ def rejection_random_sample_pytorch(
     vocab_size,
     IS_NGRAM=False,
 ):
+    """
+    Performs batched rejection sampling for speculative decoding.
+
+    For each sequence, determines the first rejection point k where:
+        P_target(τ_i) / P_draft(τ_i) < U_i (U_i ~ Uniform(0,1))
+    
+    Tokens before k are accepted. Token k is replaced with recovered_token.
+    Tokens after k are discarded. If no rejection occurs, appends bonus_token.
+    """
+
     batch_size = output_token_ids.shape[0]
+    device = output_token_ids.device
 
-    for req_idx in range(batch_size):
-        if is_greedy[req_idx]:
-            continue
+    zero_cpu = torch.tensor([0], pin_memory=True)
+    zero_device = zero_cpu.to(device, non_blocking=True)
 
-        if req_idx == 0:
-            start_idx = 0
-        else:
-            start_idx = cu_num_draft_tokens[req_idx - 1].item()
-        end_idx = cu_num_draft_tokens[req_idx].item()
-        num_draft_tokens = end_idx - start_idx
+    # 1. Reshape flat draft arrays to [batch, max_draft_len].
+    
+    # Calculate per-batch draft token ranges.
+    cu_start = torch.cat([zero_device, cu_num_draft_tokens[:-1]])
+    cu_end = cu_num_draft_tokens
+    num_draft_per_batch = cu_end - cu_start
 
-        rejected = False
-        for pos in range(num_draft_tokens):
-            if not rejected:
-                draft_token_id = draft_token_ids[start_idx + pos].item()
+    max_draft_len = max_spec_len
+    pos_indices_cpu = torch.arange(max_draft_len, pin_memory=True)
+    pos_indices = pos_indices_cpu.to(device, non_blocking=True)[None, :]
+    valid_mask = pos_indices < num_draft_per_batch[:, None]
 
-                if IS_NGRAM:
-                    draft_prob = 1.0
-                else:
-                    draft_prob = draft_probs[start_idx + pos,
-                                             draft_token_id].item()
+    # Map batch positions to global indices in the flattened input
+    global_token_indices = cu_start[:, None] + pos_indices
+    global_token_indices = global_token_indices.clamp(0, draft_token_ids.shape[0] - 1)
+    
+    draft_tokens = draft_token_ids[global_token_indices]
 
-                target_prob = target_probs[start_idx + pos,
-                                           draft_token_id].item()
-                uniform_prob = uniform_probs[start_idx + pos].item()
+    flat_indices = global_token_indices.flatten()
+    flat_draft_tokens = draft_tokens.flatten()
 
-                if draft_prob > 0 and target_prob / draft_prob >= uniform_prob:
-                    token_id = draft_token_id
-                else:
-                    rejected = True
-                    token_id = recovered_token_ids[start_idx + pos].item()
+    # 2. Calculate P_draft and P_target for the Acceptance Check.
+
+    if IS_NGRAM:
+        ones_cpu = torch.ones(1, pin_memory=True, dtype=torch.float32)
+        draft_token_probs = ones_cpu.to(device, non_blocking=True).expand_as(draft_tokens)
+    else:
+        flat_draft_probs = draft_probs[flat_indices, flat_draft_tokens]
+        draft_token_probs = flat_draft_probs.view(batch_size, max_draft_len)
 
-                output_token_ids[req_idx, pos] = token_id
+    flat_target_probs = target_probs[flat_indices, flat_draft_tokens]
+    target_token_probs = flat_target_probs.view(batch_size, max_draft_len)
 
-        if not rejected:
-            bonus_token_id = bonus_token_ids[req_idx].item()
-            output_token_ids[req_idx, num_draft_tokens] = bonus_token_id
+    uniform_token_probs = uniform_probs[global_token_indices]
+    
+    recovered_tokens = recovered_token_ids[global_token_indices]
 
+    # 3. Determine Acceptance and the Rejection Point.
+
+    zero_threshold_cpu = torch.tensor([0.0], pin_memory=True, dtype=torch.float32)
+    zero_threshold = zero_threshold_cpu.to(device, non_blocking=True)
+
+    # Acceptance criterion: P_target/P_draft ≥ U (with P_draft > 0 safeguard)
+    first_acceptance = (draft_token_probs > zero_threshold) & (
+        target_token_probs / draft_token_probs >= uniform_token_probs
+    )
+
+    # The first index where rejection occurs.
+    rejection_mask = (~first_acceptance) & valid_mask
+
+    default_pos_cpu = torch.full([batch_size, 1], max_draft_len, pin_memory=True)
+    default_pos = default_pos_cpu.to(device, non_blocking=True)
+
+    first_reject_pos = torch.where(
+        rejection_mask.any(dim=1, keepdim=True),
+        rejection_mask.float().argmax(dim=1, keepdim=True),
+        default_pos
+    )
+    
+    # 4. Apply Masks and Update Output Tokens
+
+    after_rejection = pos_indices >= first_reject_pos
+    after_rejection = after_rejection & valid_mask
+    
+    final_acceptance = first_acceptance & (~after_rejection)
+    
+    non_greedy_seqs = ~is_greedy
+    
+    # The positions before the rejection point.
+    accepted_positions = non_greedy_seqs[:, None] & valid_mask & (~after_rejection)
+
+    # The rejection point. 
+    rejection_point = (pos_indices == first_reject_pos) & valid_mask & non_greedy_seqs[:, None]
+    
+    final_update_positions = accepted_positions | rejection_point
+    
+    # Select the final tokens:
+    #   - At k: recovered token.
+    #   - Before k: accepted draft token.
+    final_tokens = torch.where(
+        rejection_point,
+        recovered_tokens,
+        torch.where(final_acceptance, draft_tokens, output_token_ids[:, :max_draft_len])
+    )
+
+    #   Otherwise: keep the existing output.
+    output_token_ids[:, :max_draft_len] = torch.where(
+        final_update_positions,
+        final_tokens,
+        output_token_ids[:, :max_draft_len]
+    )
+
+    # 5. Handle the Bonus Token
+
+    # Insert bonus tokens only when no rejection occurs.
+    no_rejection = first_reject_pos.squeeze(1) >= num_draft_per_batch
+    should_add_bonus = non_greedy_seqs & no_rejection
+
+    bonus_positions = num_draft_per_batch
+    seq_len = output_token_ids.shape[1]
+    
+    all_positions_cpu = torch.arange(seq_len, pin_memory=True)
+    all_positions = all_positions_cpu.to(device, non_blocking=True)[None, :]
+
+    batch_bonus_positions = bonus_positions[:, None]
+
+    # Boundary check for bonus tokens.
+    max_spec_len_cpu = torch.tensor([max_spec_len], pin_memory=True)
+    max_spec_len_device = max_spec_len_cpu.to(device, non_blocking=True)
+
+    valid_bonus_pos = bonus_positions < (max_spec_len_device + 1)
+    final_bonus_mask = should_add_bonus & valid_bonus_pos
+
+    bonus_pos_match = (all_positions == batch_bonus_positions)
+    bonus_pos_mask = bonus_pos_match & final_bonus_mask[:, None]
+
+    # Insert the bonus tokens.
+    bonus_values_expanded = bonus_token_ids.view(-1, 1).expand(-1, seq_len)
+    output_token_ids[:] = torch.where(bonus_pos_mask, bonus_values_expanded, output_token_ids)
 
 def expand_pytorch(
     output_ptr,  # [num_tokens]
@@ -468,37 +565,33 @@ def sample_recovered_tokens_pytorch(
     vocab_size,
     IS_NGRAM=False,
 ):
+    device = target_probs.device
+    num_tokens = target_probs.shape[0]
     batch_size = len(cu_num_draft_tokens)
 
-    for req_idx in range(batch_size):
-        start_idx = 0 if req_idx == 0 else cu_num_draft_tokens[req_idx - 1]
-        end_idx = cu_num_draft_tokens[req_idx]
-        num_draft_tokens = end_idx - start_idx
-
-        for pos in range(num_draft_tokens):
-            token_idx = start_idx + pos
-
-            if IS_NGRAM:
-                draft_token_id = draft_token_ids[token_idx]
-                orig_prob = target_probs[token_idx, draft_token_id].item()
-                target_probs[token_idx, draft_token_id] = 0
-                prob = target_probs[token_idx].clone()
-            else:
-                draft_p = draft_probs[token_idx].clone()
-                target_p = target_probs[token_idx].clone()
-                prob = torch.maximum(target_p - draft_p,
-                                     torch.tensor(0.0, device=target_p.device))
-
-            q_values = torch.full((vocab_size, ),
-                                  float('-inf'),
-                                  device=q.device)
-            q_values[:vocab_size] = q[req_idx, :vocab_size]
-
-            recovered_id = torch.argmax(prob / q_values).item()
-            output_token_ids[token_idx] = recovered_id
-
-            if IS_NGRAM:
-                target_probs[token_idx, draft_token_id] = orig_prob
+    if IS_NGRAM:
+        prob = target_probs.clone()
+        token_indices = torch.arange(num_tokens, device=device)
+        prob[token_indices, draft_token_ids] = 0.0
+    else:
+        prob = torch.relu(target_probs - draft_probs)
+    
+    index_device = cu_num_draft_tokens.device
+
+    num_draft_tokens_per_req = torch.diff(
+        cu_num_draft_tokens,
+        prepend=torch.tensor([0], device=index_device)
+    )
+
+    request_indices = torch.repeat_interleave(
+        torch.arange(batch_size, device=index_device),
+        num_draft_tokens_per_req
+    ).to(device)
+
+    q_values = q[request_indices]
+    recovered_ids = torch.argmax(prob / q_values, dim=1)
+
+    output_token_ids[:num_tokens] = recovered_ids
 
 
 rs.expand_batch_to_tokens = expand_batch_to_tokens
-- 
2.28.0.windows.1

